{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): Only clang++ is supported. With g++, we end up with strange g++/OSX bugs.\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne import nonlinearities\n",
    "from lasagne import init\n",
    "from lasagne.utils import unroll_scan\n",
    "\n",
    "from lasagne.layers import MergeLayer, Layer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we generate the same data every time by fixing the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is the (x,y) position of a ball as it ricochets around in a box of size (xmin,xmax,ymin,ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin,xmax,ymin,ymax = (-1,1,-1,1)\n",
    "lower_bound = np.array([xmin,ymin])\n",
    "upper_bound = np.array([xmax,ymax])\n",
    "\n",
    "ndims = 2\n",
    "nsteps = 40\n",
    "nsequences = 40\n",
    "\n",
    "def bounce_ball(pos,vel):\n",
    "    for n in range(1,nsteps):\n",
    "        pos_next = pos[n-1,:] + vel\n",
    "        within_boundary = [lower_bound[d] < pos_next[d] < upper_bound[d] for d in range(ndims)]\n",
    "        if not all(within_boundary):\n",
    "            vel[~np.array(within_boundary)] *= -1\n",
    "        pos[n,:] = pos[n-1,:]+vel\n",
    "    return pos\n",
    "\n",
    "\n",
    "x = np.zeros((nsequences,nsteps,1,2))\n",
    "for s in range(nsequences):\n",
    "    x0 = np.random.rand()*(xmax-xmin)+xmin\n",
    "    y0 = np.random.rand()*(ymax-ymin)+ymin\n",
    "    x[s,0,0,:] = np.array([x0,y0])\n",
    "\n",
    "    vel = np.random.rand(2)\n",
    "    vel = vel/(10*np.linalg.norm(vel))\n",
    "    x[s,:,0,:] = bounce_ball(x[s,:,0,:],vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 1, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(*x[5,:,:].T,'.')\n",
    "#hhh = T.zeros([3,4,5,6])\n",
    "#print(hhh.shape.eval()[1])\n",
    "xTest = x #np.reshape(x[:,1:,:,:],(nsequences,nsteps-1,2,ndims))\n",
    "xTest.shape\n",
    "#yTest = np.reshape(x[:,0:(nsteps-1),:,:],(nsequences,(nsteps-1),2,ndims))\n",
    "#yTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pos = x[3]\n",
    "#for n in range(nsteps):\n",
    "#    display.display(plt.clf());\n",
    "#    #plt.plot(*pos[n,:].T,'.',color='b');\n",
    "#    plt.xlim(-1,1);\n",
    "#    plt.ylim(-1,1);\n",
    "#    display.clear_output(wait=True);\n",
    "#    display.display(plt.gcf());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasagne's GRUlayer expects a three-dimensional input (batch_size,seq_len,dimension_of_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is supposed to be the next frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = np.zeros([5,9,100,50])\n",
    "#y = np.zeros([5,9,100,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasagne also requires float32's for some reason (probably to save space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTest = xTest.astype(np.float32)\n",
    "yTest = yTest.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Lasagne won't work with singular hidden units, the model has two, but we only use one in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_INPUTS = 1\n",
    "NUM_UNITS_ENC = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#symbolic theano variables. Note that we are using imatrix for X since it goes into the embedding layer\n",
    "x_sym = T.matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sym = T.ftensor4()\n",
    "y_sym = T.ftensor4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   3   6  10  15  21  28  36  45  55  66  78  91 105]\n",
      "[0, 1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#imported example to understand theno scan\n",
    "#\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "up_to = T.iscalar(\"up_to\")\n",
    "\n",
    "# define a named function, rather than using lambda\n",
    "def accumulate_by_adding(arange_val, sum_to_date):\n",
    "    return sum_to_date + arange_val\n",
    "    #return arange_val\n",
    "seq = T.arange(up_to)\n",
    "\n",
    "# An unauthorized implicit downcast from the dtype of 'seq', to that of\n",
    "# 'T.as_tensor_variable(0)' which is of dtype 'int8' by default would occur\n",
    "# if this instruction were to be used instead of the next one:\n",
    "# outputs_info = T.as_tensor_variable(0)\n",
    "\n",
    "outputs_info = T.as_tensor_variable(np.asarray(0, seq.dtype))\n",
    "scan_result, scan_updates = theano.scan(fn=accumulate_by_adding,\n",
    "                                        outputs_info=outputs_info,\n",
    "                                        sequences=seq)\n",
    "triangular_sequence = theano.function(inputs=[up_to], outputs=scan_result)\n",
    "\n",
    "# test\n",
    "some_num = 15\n",
    "print(triangular_sequence(some_num))\n",
    "print([n * (n + 1) // 2 for n in range(some_num)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try with a batch size of one, to test the code with long waiting times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_first = x[0:10,:].astype(np.float32)\n",
    "#y_first = np.zeros([10,99,4]).astype(np.float32);\n",
    "#y_first[0:10,:,0:1] = y[0:10,:,0:1].astype(np.float32);\n",
    "#y_first = y[0:10,:,0].astype(np.float32)\n",
    "\n",
    "#x_first = x[:,:].astype(np.float32)\n",
    "#y_first = np.zeros([10,99,4]).astype(np.float32);\n",
    "#y_first[:,:,0:1] = y[:,:,0:1].astype(np.float32);\n",
    "\n",
    "#y = y_first;\n",
    "#x = x_first;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_first.shape,y_first.shape\n",
    "class DummyLayer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, shape, **kwargs):\n",
    "        super(DummyLayer, self).__init__(incoming, **kwargs)\n",
    "        self._shape = shape\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        #stupid workaround due to stupid python code-checking... Wish this was C++...\n",
    "        return input.sum()*T.zeros(self._shape)\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (self._shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OurLayer(lasagne.layers.CustomRecurrentLayer):\n",
    "# Create single recurrent computation step function\n",
    "    def __init__(self, incoming,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            hid_init=lasagne.init.Constant(0.),\n",
    "            backwards=False,\n",
    "            learn_init=False,\n",
    "            gradient_steps=-1,\n",
    "            grad_clipping=0,\n",
    "            unroll_scan=False,\n",
    "            precompute_input=True,\n",
    "            mask_input=None,\n",
    "            only_return_final=False,\n",
    "            num_layers=2,\n",
    "            name=None,\n",
    "            **kwargs):\n",
    "        if isinstance(incoming, tuple):\n",
    "            input_shape = incoming\n",
    "        else:\n",
    "            input_shape = incoming.output_shape\n",
    "        \n",
    "        #l_in_to_hid = lasagne.layers.DenseLayer(lasagne.layers.InputLayer((None,) + input_shape[2:]),num_units=num_layers)\n",
    "        #l_hid_to_hid = lasagne.layers.DenseLayer(lasagne.layers.InputLayer(([None], num_layers)),num_units=num_layers)\n",
    "        \n",
    "        l_in_to_hid  = DummyLayer(lasagne.layers.InputLayer( ((input_shape[0]*input_shape[1]),input_shape[2:]              ) ),shape=((input_shape[0]*input_shape[1]), (input_shape[2]), (input_shape[3]), 3, (num_layers)))\n",
    "        l_hid_to_hid = DummyLayer(lasagne.layers.InputLayer( ((input_shape[0]*input_shape[1]), (input_shape[2]), (input_shape[3]), 3, (num_layers)) ),shape=((input_shape[0]*input_shape[1]), (input_shape[2]), (input_shape[3]), 3, (num_layers)))\n",
    "        \n",
    "        #make sure our parent works correctly:\n",
    "        lasagne.layers.CustomRecurrentLayer.__init__(self,incoming,input_to_hidden=l_in_to_hid,hidden_to_hidden=l_hid_to_hid,name=name)\n",
    "        \n",
    "        #l_test = lasagne.layers.ReshapeLayer(lasagne.layers.InputLayer((None,) + input_shape[2:]),shape=[100,50,5,9])\n",
    "        \n",
    "        #print(l_hid_to_hid.input_shape)\n",
    "        #print(l_hid_to_hid.output_shape)\n",
    "        \n",
    "        #print(l_in_to_hid.input_shape)\n",
    "        #print(l_in_to_hid.output_shape)\n",
    "        \n",
    "        self.num_batch = input_shape[0]\n",
    "        \n",
    "        self.WE = []\n",
    "        self.WR = []\n",
    "        self.WRR = []\n",
    "        self.WA = []\n",
    "        for j in range(0,num_layers):\n",
    "            name='wE'+'_layer_'+str(j)\n",
    "            self.WE.append( self.add_param(lasagne.init.Normal(0.01),shape=((input_shape[2]*input_shape[3],input_shape[2]*input_shape[3])),name=name) )\n",
    "            name='wR'+'_layer_'+str(j)\n",
    "            self.WR.append( self.add_param(lasagne.init.Normal(0.01),shape=((input_shape[2]*input_shape[3],input_shape[2]*input_shape[3])),name=name) )\n",
    "            if (j == num_layers-1):\n",
    "                self.WRR.append(1)\n",
    "            else:\n",
    "                name='wRR'+'_layer_'+str(j)\n",
    "                self.WRR.append( self.add_param(lasagne.init.Normal(0.01),shape=((input_shape[2]*input_shape[3],input_shape[2]*input_shape[3])),name=name) )\n",
    "            name='wA'+'_layer_'+str(j)\n",
    "            self.WA.append( self.add_param(lasagne.init.Normal(0.01),shape=((input_shape[2]*input_shape[3],input_shape[2]*input_shape[3])),name=name) )\n",
    "        \n",
    "        #print(l_op1.input_shape)\n",
    "        #print(l_op1.output_shape)\n",
    "        \n",
    "        #print( (None,) + input_shape[2:] )\n",
    "        \n",
    "        #initialize our own data:\n",
    "        self.num_layers=num_layers\n",
    "        self.input_shape = input_shape\n",
    "        #figure out where to init hid_previous, hid_now\n",
    "        self.has_inited_step = 0\n",
    "        \n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute this layer's output function given a symbolic input variable.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : list of theano.TensorType\n",
    "            `inputs[0]` should always be the symbolic input variable.  When\n",
    "            this layer has a mask input (i.e. was instantiated with\n",
    "            `mask_input != None`, indicating that the lengths of sequences in\n",
    "            each batch vary), `inputs` should have length 2, where `inputs[1]`\n",
    "            is the `mask`.  The `mask` should be supplied as a Theano variable\n",
    "            denoting whether each time step in each sequence in the batch is\n",
    "            part of the sequence or not.  `mask` should be a matrix of shape\n",
    "            ``(n_batch, n_time_steps)`` where ``mask[i, j] = 1`` when ``j <=\n",
    "            (length of sequence i)`` and ``mask[i, j] = 0`` when ``j > (length\n",
    "            of sequence i)``. When the hidden state of this layer is to be\n",
    "            pre-filled (i.e. was set to a :class:`Layer` instance) `inputs`\n",
    "            should have length at least 2, and `inputs[-1]` is the hidden state\n",
    "            to prefill with.\n",
    "        Returns\n",
    "        -------\n",
    "        layer_output : theano.TensorType\n",
    "            Symbolic output variable.\n",
    "        \"\"\"\n",
    "        # Retrieve the layer input\n",
    "        input = inputs[0]\n",
    "        # Retrieve the mask when it is supplied\n",
    "        mask = None\n",
    "        hid_init = None\n",
    "        if self.mask_incoming_index > 0:\n",
    "            mask = inputs[self.mask_incoming_index]\n",
    "        if self.hid_init_incoming_index > 0:\n",
    "            hid_init = inputs[self.hid_init_incoming_index]\n",
    "\n",
    "        # Input should be provided as (n_batch, n_time_steps, n_features)\n",
    "        # but scan requires the iterable dimension to be first\n",
    "        # So, we need to dimshuffle to (n_time_steps, n_batch, n_features)\n",
    "        input = input.dimshuffle(1, 0, *range(2, input.ndim))\n",
    "        seq_len, num_batch = input.shape[0], input.shape[1]\n",
    "\n",
    "        #do not precompute!\n",
    "        self.precompute_input=False\n",
    "        \n",
    "        if self.precompute_input:\n",
    "            # Because the input is given for all time steps, we can precompute\n",
    "            # the inputs to hidden before scanning. First we need to reshape\n",
    "            # from (seq_len, batch_size, trailing dimensions...) to\n",
    "            # (seq_len*batch_size, trailing dimensions...)\n",
    "            # This strange use of a generator in a tuple was because\n",
    "            # input.shape[2:] was raising a Theano error\n",
    "            trailing_dims = tuple(input.shape[n] for n in range(2, input.ndim))\n",
    "            input = T.reshape(input, (seq_len*num_batch,) + trailing_dims)\n",
    "            input = helper.get_output(\n",
    "                self.input_to_hidden, input, **kwargs)\n",
    "\n",
    "            # Reshape back to (seq_len, batch_size, trailing dimensions...)\n",
    "            trailing_dims = tuple(input.shape[n] for n in range(1, input.ndim))\n",
    "            input = T.reshape(input, (seq_len, num_batch) + trailing_dims)\n",
    "\n",
    "        # We will always pass the hidden-to-hidden layer params to step\n",
    "        non_seqs = helper.get_all_params(self.hidden_to_hidden)\n",
    "        # When we are not precomputing the input, we also need to pass the\n",
    "        # input-to-hidden parameters to step\n",
    "        if not self.precompute_input:\n",
    "            non_seqs += helper.get_all_params(self.input_to_hidden)\n",
    "\n",
    "        # Create single recurrent computation step function\n",
    "        def step(input_n, hid_previous, *args):\n",
    "            #assume input_n: array[N][M]\n",
    "            #assume hid_previous: [N][M][3][num_layers]\n",
    "\n",
    "            #print('hello world')\n",
    "            #print(input_n.shape)\n",
    "            def eval_fun_bot(hid_now):\n",
    "                return hid_now\n",
    "    \n",
    "            def eval_fun(hid_now):\n",
    "                return hid_now\n",
    "\n",
    "            hid_now = hid_previous;\n",
    "            #hid_now[:,:,:] = 2*hid_previous[:,:,:]\n",
    "            \n",
    "            #print(input_n.shape.eval())\n",
    "            #print(hid_previous.shape)\n",
    "            \n",
    "            #assume hid_previous[:][:][0][i]: R on i'th layers.\n",
    "            #assume hid_previous[:][:][1][i]: E on i'th layers.\n",
    "            #assume hid_previous[:][:][2][i]: A on i'th layers.\n",
    "            ##assume hid_previous[:][:][3][1]: Output image from previous state.\n",
    "\n",
    "            #N = hhh.shape.eval()[0]\n",
    "            #M = hhh.shape.eval()[1]\n",
    "\n",
    "            #if (self.has_inited_step == 0):\n",
    "            #    self.has_inited_step = 1\n",
    "#            hid_previous = T.zeros([M,N,3,self.num_layers])\n",
    "#\n",
    "#            hid_now = T.zeros([M,N,3,self.num_layers])\n",
    "\n",
    "            #testing/twillight zone:\n",
    "            #hid_now = T.set_subtensor( (hid_now[5,99,49,2,self.num_layers-1]), 1)\n",
    "            #input = input_n\n",
    "            #input = T.set_subtensor( (input_n[:,1000,100]), 1)\n",
    "        \n",
    "#\n",
    "#            #perform top to bottom operations:\n",
    "            for i in range(self.num_layers-1,-1,-1):\n",
    "                #add data from layers:\n",
    "                if (i == (self.num_layers-1)):\n",
    "                    for j in range(0,self.num_batch):\n",
    "                        t1 = T.reshape( T.dot(self.WE[i],T.reshape(hid_previous[j,:,:,1,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        t2 = T.reshape( T.dot(self.WR[i],T.reshape(hid_previous[j,:,:,0,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        hid_now = T.set_subtensor( hid_now[j,:,:,0,i] ,  t1+t2 )\n",
    "                else:\n",
    "                    for j in range(0,self.num_batch):\n",
    "                        t1 = T.reshape( T.dot(self.WE[i],T.reshape(hid_previous[j,:,:,1,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        t2 = T.reshape( T.dot(self.WR[i],T.reshape(hid_previous[j,:,:,0,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        t3 = T.reshape( T.dot(self.WRR[i],T.reshape(hid_previous[j,:,:,0,i+1],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )   \n",
    "                        hid_now = T.set_subtensor( hid_now[j,:,:,0,i] ,  t1+t2+t3 )\n",
    "                \n",
    "                #    hid_now = T.set_subtensor( (hid_now[:,:,:,0,i]) , hid_previous[:,:,:,0,i] + hid_previous[:,:,:,1,i] + hid_now[:,:,:,0,i+1])\n",
    "\n",
    "            #perform bottom to top operations:\n",
    "            for i in range(0,self.num_layers):\n",
    "                #perform approximations.\n",
    "                if (i==0):\n",
    "                    for j in range(0,self.num_batch):\n",
    "                        #calculate A\n",
    "                        t1 = T.reshape( T.dot(self.WA[i],T.reshape(hid_previous[j,:,:,0,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        hid_now = T.set_subtensor(hid_now[:,:,:,2,i], t1 )\n",
    "                        #calculate errors:\n",
    "                        T.set_subtensor(hid_now[:,:,:,1,i], hid_now[:,:,:,2,i] - input_n[:,:,:])\n",
    "                else:\n",
    "                    for j in range(0,self.num_batch):\n",
    "                        t1 = T.reshape( T.dot(self.WA[i],T.reshape(hid_previous[j,:,:,0,i],((self.input_shape[2]*self.input_shape[3],1)) )), ((self.input_shape[2],self.input_shape[3])) )\n",
    "                        hid_now = T.set_subtensor(hid_now[:,:,:,2,i], t1 )\n",
    "                        #calculate errors:\n",
    "                        hid_now = T.set_subtensor(hid_now[:,:,:,1,i], hid_now[:,:,:,2,i] - hid_now[:,:,:,2,i-1])\n",
    "\n",
    "            return hid_now\n",
    "\n",
    "        def step_masked(input_n, mask_n, hid_previous, *args):\n",
    "            # Skip over any input with mask 0 by copying the previous\n",
    "            # hidden state; proceed normally for any input with mask 1.\n",
    "            hid = step(input_n, hid_previous, *args)\n",
    "            hid_out = T.switch(mask_n, hid, hid_previous)\n",
    "            return [hid_out]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.dimshuffle(1, 0, 'x')\n",
    "            sequences = [input, mask]\n",
    "            step_fun = step_masked\n",
    "        else:\n",
    "            sequences = input\n",
    "            step_fun = step\n",
    "\n",
    "        if not isinstance(self.hid_init, Layer):\n",
    "            # The code below simply repeats self.hid_init num_batch times in\n",
    "            # its first dimension.  Turns out using a dot product and a\n",
    "            # dimshuffle is faster than T.repeat.\n",
    "            dot_dims = (list(range(1, self.hid_init.ndim - 1)) +\n",
    "                        [0, self.hid_init.ndim - 1])\n",
    "            hid_init = T.dot(T.ones((num_batch, 1)),\n",
    "                             self.hid_init.dimshuffle(dot_dims))\n",
    "\n",
    "        if self.unroll_scan:\n",
    "            # Retrieve the dimensionality of the incoming layer\n",
    "            input_shape = self.input_shapes[0]\n",
    "            # Explicitly unroll the recurrence instead of using scan\n",
    "            hid_out = unroll_scan(\n",
    "                fn=step_fun,\n",
    "                sequences=sequences,\n",
    "                outputs_info=[hid_init],\n",
    "                go_backwards=self.backwards,\n",
    "                non_sequences=non_seqs,\n",
    "                n_steps=input_shape[1])[0]\n",
    "        else:\n",
    "            # Scan op iterates over first dimension of input and repeatedly\n",
    "            # applies the step function\n",
    "            hid_out = theano.scan(\n",
    "                fn=step_fun,\n",
    "                sequences=sequences,\n",
    "                go_backwards=self.backwards,\n",
    "                outputs_info=[hid_init],\n",
    "                non_sequences=non_seqs,\n",
    "                truncate_gradient=self.gradient_steps,\n",
    "                strict=False)[0]\n",
    "\n",
    "        # When it is requested that we only return the final sequence step,\n",
    "        # we need to slice it out immediately after scan is applied\n",
    "        if self.only_return_final:\n",
    "            hid_out = hid_out[-1]\n",
    "        else:\n",
    "            # dimshuffle back to (n_batch, n_time_steps, n_features))\n",
    "            hid_out = hid_out.dimshuffle(1, 0, *range(2, hid_out.ndim))\n",
    "\n",
    "            # if scan is backward reverse the output\n",
    "            if self.backwards:\n",
    "                hid_out = hid_out[:, ::-1]\n",
    "\n",
    "        return hid_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer((nsequences,nsteps,1,2), input_var = x_sym)\n",
    "\n",
    "l_custom = OurLayer(l_in,num_layers=1, name='l_custom')\n",
    "\n",
    "#l_in2 = lasagne.layers.InputLayer((5,9,100,50), input_var = x_sym)\n",
    "#l_dummy = DummyLayer(l_in2,shape=(5,9,100,50))\n",
    "\n",
    "batch_size = 5,\n",
    "seq_len = 9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#total error:\n",
    "#l_out_total = lasagne.layers.ConcatLayer([l_out0],axis=3);\n",
    "#l_out_total = lasagne.layers.ConcatLayer([l_out, l_out],axis=2);\n",
    "\n",
    "# make sure output layer's range matches the range of input, fx with linear output layer\n",
    "print(lasagne.layers.get_output(l_custom, inputs={l_in: x_sym}).eval({x_sym: xTest}).shape)\n",
    "#print(lasagne.layers.get_output(l_dummy, inputs={l_in: x_sym}).eval({x_sym: x}).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[l_custom.hid_init, l_custom.wE_layer_0, l_custom.wR_layer_0, l_custom.wA_layer_0]\n"
     ]
    }
   ],
   "source": [
    "print(lasagne.layers.get_all_params(l_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost function is mean absolute difference between predicted and actual next step.\n",
    "predicted_values = lasagne.layers.get_output(l_custom)\n",
    "\n",
    "cost = T.mean(abs(predicted_values[:,:,:,:,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.eval({x_sym: xTest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_parameters = lasagne.layers.get_all_params([l_custom], trainable=True)\n",
    "#all_parameters = all_parameters[0:2]\n",
    "#print(all_parameters)\n",
    "\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-3,3) for g in T.grad(cost, all_parameters)]\n",
    "#updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.001)\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(l_custom.wE_layer_0, (2, 2))\n",
      "(l_custom.wR_layer_0, (2, 2))\n",
      "(l_custom.wA_layer_0, (2, 2))\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('The following error happened while compiling the node', for{cpu,grad_of_scan_fn}(Elemwise{sub,no_inplace}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{::int64}.0, Alloc.0, Alloc.0, Alloc.0, Subtensor{int64}.0, l_custom.wE_layer_0, l_custom.wR_layer_0, l_custom.wA_layer_0), '\\n', 'Non-unit value on shape on a broadcastable dimension.', (0, 0, 0, 0, 0), (False, True, False, False, True), 'Container name \"None\"')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b0ed5f452c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compile training function (in C or somesuch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_sym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FAST_COMPILE'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#y_sym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    482\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1787\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1653\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1654\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                                                  \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                                                  no_recycling))\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0;31m# We don't want all ops maker to think about lazy Ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    855\u001b[0m                                \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                                \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m                                on_unused_input='ignore')\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;31m# Analyse the compile inner function to determine which inputs and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    482\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1787\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         fn = self.function_builder(_fn, _i, _o, self.indices, self.outputs,\n\u001b[1;32m   1668\u001b[0m                                    \u001b[0mdefaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_single\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                                    self.return_none, self.output_keys, self)\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, input_storage, output_storage, indices, outputs, defaults, unpack_single, return_none, output_keys, maker)\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrefeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    477\u001b[0m                                                            **kwargs)\n\u001b[1;32m    478\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anfam/anaconda/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 raise TypeError(\"Non-unit value on shape on a broadcastable\"\n\u001b[0;32m--> 195\u001b[0;31m                                 \" dimension.\", data.shape, self.broadcastable)\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         if (self.filter_checks_isfinite and\n",
      "\u001b[0;31mTypeError\u001b[0m: ('The following error happened while compiling the node', for{cpu,grad_of_scan_fn}(Elemwise{sub,no_inplace}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{::int64}.0, Alloc.0, Alloc.0, Alloc.0, Subtensor{int64}.0, l_custom.wE_layer_0, l_custom.wR_layer_0, l_custom.wA_layer_0), '\\n', 'Non-unit value on shape on a broadcastable dimension.', (0, 0, 0, 0, 0), (False, True, False, False, True), 'Container name \"None\"')"
     ]
    }
   ],
   "source": [
    "# compile training function (in C or somesuch)\n",
    "train_func = theano.function([x_sym], [cost, predicted_values], updates=updates,mode='FAST_COMPILE') #y_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "csts = []\n",
    "preds = []\n",
    "num_epochs = 300\n",
    "#pred = np.zeros(y.shape).astype(np.float32)\n",
    "while c < num_epochs:\n",
    "    cst, pred = train_func(x,y)\n",
    "    pred = pred.astype(np.float32)\n",
    "    csts.append(cst)\n",
    "    preds.append(pred)\n",
    "    c += 1\n",
    "    #print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#while c < num_epochs*2:\n",
    "#    cst, pred = train_func(x,pred[:,:,2,y)\n",
    "#    csts.append(cst)\n",
    "#    preds.append(pred)\n",
    "#    c += 1\n",
    "#    #print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(csts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds[-1][:,98,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(csts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare prediction generated by a randomly initialized network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_first[0,:,0].T.flatten(),preds[0][0,:,0].T.flatten(),'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to prediction by trained network (ideal is a straight line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_first[0,:,0].T.flatten(),preds[-1][0,:,0].T.flatten(),'*');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few sample sequence comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_idxs = np.random.choice(10,9,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = 0\n",
    "for i in sequence_idxs:\n",
    "    cc += 1\n",
    "    plt.subplot(3,3,cc)\n",
    "    plt.plot(y_first[i,:,0].T)\n",
    "    plt.plot(preds[-1][i,:,0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
